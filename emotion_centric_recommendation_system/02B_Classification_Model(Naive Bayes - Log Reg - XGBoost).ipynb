{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"http://imgur.com/1ZcRyrc.png\" style=\"float: left; margin: 5px; height: 50px\"> \n",
    "\n",
    "#   Personalizing Music Video Recommendations with Emotional Intelligence\n",
    "\n",
    "> Capstone Project: Lionel Foo\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### <b> Notebook: 02B Classification Model (Multiclass Classification - Naive Bayes, Logistic Regression, XGBoost) </b>\n",
    "\n",
    "#### Naive Bayes (MultinomialNB)\n",
    "**Rationale:**\n",
    "Naive Bayes is a probabilistic algorithm based on Bayes' theorem. It's particularly suitable for text classification tasks due to its simplicity, efficiency, and effectiveness, especially with a relatively small dataset. In the context of emotional text classification, Naive Bayes can capture the likelihood of specific words or features contributing to each emotion class independently, making it robust for sentiment analysis.\n",
    "\n",
    "#### Logistic Regression\n",
    "**Rationale:**\n",
    "Logistic Regression is a linear model used for binary and multi-class classification. In the case of emotional text classification, it can be effective in modeling the relationship between the input features (words in the text) and the probability of belonging to each emotion class. Logistic Regression is known for its simplicity and interpretability.\n",
    "\n",
    "#### XGBoostClassifier\n",
    "**Rationale:**\n",
    "XGBoost is an ensemble learning algorithm that combines the strengths of decision trees. XGBoost builds an ensemble of decision trees sequentially, where each tree corrects the errors of the previous ones. It uses a gradient boosting framework to minimize a loss function and optimize predictive performance. Each tree corrects the errors of the previous ones, leading to a strong predictive model. It has high predictive performance and ability to handle complex relationships in data. In emotional text classification, where capturing nuanced patterns is crucial, XGBoost can provide superior predictive power.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "<b> 1. Import Libraries</b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to /Users/lionel/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "#!pip install xgboost\n",
    "\n",
    "# Imports: standard\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "import nltk\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "nltk.download('wordnet')\n",
    "from nltk.corpus import stopwords\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from xgboost import XGBClassifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "---\n",
    "\n",
    "<b> 2. Import dataframe and assess imported data </b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>label</th>\n",
       "      <th>emotion_name</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>i just feel really helpless and heavy hearted</td>\n",
       "      <td>4</td>\n",
       "      <td>Fear</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>i have enjoyed being able to slouch about rela...</td>\n",
       "      <td>0</td>\n",
       "      <td>Sadness</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>i gave up my internship with the dmrg and am f...</td>\n",
       "      <td>4</td>\n",
       "      <td>Fear</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text  label emotion_name\n",
       "0      i just feel really helpless and heavy hearted      4         Fear\n",
       "1  i have enjoyed being able to slouch about rela...      0      Sadness\n",
       "2  i gave up my internship with the dmrg and am f...      4         Fear"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Import data\n",
    "df = pd.read_csv(\"Data/emotions_processed_dataset.csv\")\n",
    "df.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 380365 entries, 0 to 380364\n",
      "Data columns (total 3 columns):\n",
      " #   Column        Non-Null Count   Dtype \n",
      "---  ------        --------------   ----- \n",
      " 0   text          380365 non-null  object\n",
      " 1   label         380365 non-null  int64 \n",
      " 2   emotion_name  380365 non-null  object\n",
      "dtypes: int64(1), object(2)\n",
      "memory usage: 8.7+ MB\n"
     ]
    }
   ],
   "source": [
    "# Summary of DataFrame information\n",
    "df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "---\n",
    "\n",
    "<b> 2. Prepare Data for Modelling </b>\n",
    "\n",
    "Outline\n",
    "* (a) Evaluate Class Imbalance\n",
    "* (b) Lemmatize 'text' Column\n",
    "* (c) Perform Train-Test-Split\n",
    "* (d) Address Class Imbalance\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b> (a) Evaluate Class Imbalance </b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "== Examine Class Imbalance ==\n",
      "\n",
      "= Counts =\n",
      "\n",
      "   label emotion_name   count\n",
      "0      0      Sadness  118152\n",
      "1      1          Joy  134709\n",
      "2      2         Love   29410\n",
      "3      3        Anger   54597\n",
      "4      4         Fear   43497\n",
      "\n",
      "= Proportions =\n",
      "\n",
      "label  emotion_name\n",
      "0      Sadness         0.310628\n",
      "1      Joy             0.354157\n",
      "2      Love            0.077320\n",
      "3      Anger           0.143538\n",
      "4      Fear            0.114356\n",
      "dtype: float64\n"
     ]
    }
   ],
   "source": [
    "# Examine Class Imbalance\n",
    "print(\"== Examine Class Imbalance ==\\n\")\n",
    "\n",
    "# Counts\n",
    "class_counts = df.groupby(['label', 'emotion_name']).size().reset_index(name='count')\n",
    "print(\"= Counts =\\n\")\n",
    "print(class_counts[['label', 'emotion_name', 'count']])\n",
    "\n",
    "# Proportions\n",
    "print(\"\\n= Proportions =\\n\")\n",
    "class_proportions = df.groupby(['label', 'emotion_name']).size() / len(df)\n",
    "print(class_proportions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Comments: Class Imbalance\n",
    "* Class [0]: Sadness ; Class [1]: Joy, Class [2]: Love, Class [3]: Anger, Class [4]: Fear\n",
    "* There are a disproportionately high number of \"Sadness\" (94522), \"Joy\" (107767), and \"Anger\" (43678) instances.\n",
    "* There are a disproportionately low number of \"Love\" (23528) instances.\n",
    "\n",
    "Comments: Approach to Addressing Class Imbalance\n",
    "* Undersampling (oversampling) of the most (least) numerous class to attain a count equivalent to the \"Fear\" class (as it has a count between both classes), where we'll:\n",
    "    * undersample: the most numerous classes (\"Sadness\", \"Joy\", and \"Anger\") \n",
    "    * oversample: the least numerous class (\"Love\")\n",
    "\n",
    "* To prevent \"training\" data from leaking into the \"testing\" data - the class rebalancing procedure will be applied:\n",
    "    * after the \"train-test-split\" step, and\n",
    "    * applied to the \"training\" sample only"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b> (b) Lemmatize 'text' Column </b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a lemmatizer object\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "# Define a function to perform lemmatization on a text\n",
    "def lemmatize_text(text):\n",
    "    return ' '.join([lemmatizer.lemmatize(word) for word in text.split()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply lemmatization to all rows in 'text' column\n",
    "df['text'] = df['text'].apply(lemmatize_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b> (c) Perform Train-Test-Split </b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the dataset first\n",
    "train_df, test_df = train_test_split(df, test_size=0.2, random_state=42, stratify=df[\"label\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Balance the training dataset\n",
    "class_4_size = train_df[train_df[\"label\"] == 4].shape[0]\n",
    "balanced_train_df = pd.concat([train_df[train_df[\"label\"] == label].sample(n=class_4_size, replace=True, random_state=42) for label in train_df[\"label\"].unique()], axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "label\n",
       "1    34797\n",
       "4    34797\n",
       "3    34797\n",
       "0    34797\n",
       "2    34797\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Display the count of each unique label in the 'label' column of the balanced_train_df DataFrame after balancing\n",
    "balanced_train_df['label'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(173985,) (76073,) (173985,) (76073,)\n"
     ]
    }
   ],
   "source": [
    "# Extract 'text' and 'label' columns from balanced_train_df for training\n",
    "X_train_bal = balanced_train_df['text']\n",
    "y_train_bal = balanced_train_df['label']\n",
    "# Extract 'text' and 'label' columns from test_df for testing\n",
    "X_test = test_df['text']\n",
    "y_test = test_df['label']\n",
    "# Print the shape of training and test data\n",
    "print(X_train_bal.shape, X_test.shape, y_train_bal.shape, y_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "---\n",
    "\n",
    "<b> 3. Modeling using Pipeline and GridSearch </b>\n",
    "\n",
    "Pipeline to streamline the text classification process and grid search, aiming to find the optimal hyperparameters for both CountVectorizer and Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "<b> (a) Naive Bayes model(MultinomialNB) </b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create copies of the train and test data for Naive Bayes model\n",
    "X_train_nb = X_train_bal.copy()\n",
    "y_train_nb = y_train_bal.copy()\n",
    "\n",
    "X_test_nb = X_test.copy()\n",
    "y_test_nb = y_test.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 162 candidates, totalling 810 fits\n",
      "Best Parameters:\n",
      "{'cvec__max_df': 0.4, 'cvec__max_features': 5000, 'cvec__min_df': 2, 'cvec__ngram_range': (1, 2), 'nb__alpha': 0.2}\n",
      "Grid Search took 1496.93 seconds\n",
      "Accuracy Score on Test Set: 0.9286080475332904\n",
      "Accuracy Score on Training Set: 0.9503462942207661\n",
      "Classification Report on Test Set:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "     Sadness       0.97      0.93      0.95     23630\n",
      "         Joy       0.97      0.90      0.94     26942\n",
      "        Love       0.74      0.96      0.84      5882\n",
      "       Anger       0.91      0.95      0.93     10919\n",
      "        Fear       0.88      0.96      0.92      8700\n",
      "\n",
      "    accuracy                           0.93     76073\n",
      "   macro avg       0.90      0.94      0.91     76073\n",
      "weighted avg       0.93      0.93      0.93     76073\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Create a pipeline with CountVectorizer and MultinomialNB\n",
    "pipeline_nb = Pipeline([\n",
    "    ('cvec', CountVectorizer(lowercase=True, stop_words= 'english')),\n",
    "    ('nb', MultinomialNB())\n",
    "])\n",
    "\n",
    "# Define parameter grid for grid search\n",
    "param_grid_nb = {\n",
    "    'cvec__max_features': [4000, 5000],\n",
    "    'cvec__min_df': [2, 3, 4],\n",
    "    'cvec__max_df': [0.4, 0.6, 0.8],\n",
    "    'cvec__ngram_range': [(1, 1), (1, 2), (1, 3)],\n",
    "    'nb__alpha': [0.1, 0.2, 0.3],\n",
    "}\n",
    "\n",
    "# Perform grid search with cross-validation\n",
    "grid_search_nb = GridSearchCV(pipeline_nb, param_grid_nb, cv=5, scoring='accuracy', verbose=1)\n",
    "start_time = time.time()\n",
    "grid_search_nb.fit(X_train_nb, y_train_nb)\n",
    "end_time = time.time()\n",
    "\n",
    "# Print best parameters\n",
    "best_params_nb = grid_search_nb.best_params_\n",
    "print(\"Best Parameters:\")\n",
    "print(best_params_nb)\n",
    "\n",
    "# Print computational time\n",
    "print(f\"Grid Search took {end_time - start_time:.2f} seconds\")\n",
    "\n",
    "# Print accuracy score for the test set\n",
    "y_pred_nb_test = grid_search_nb.best_estimator_.predict(X_test_nb)\n",
    "accuracy_nb_test = accuracy_score(y_test_nb, y_pred_nb_test)\n",
    "print(\"Accuracy Score on Test Set:\", accuracy_nb_test)\n",
    "\n",
    "# Print accuracy score for the training set\n",
    "y_pred_nb_train = grid_search_nb.best_estimator_.predict(X_train_nb)\n",
    "accuracy_nb_train = accuracy_score(y_train_nb, y_pred_nb_train)\n",
    "print(\"Accuracy Score on Training Set:\", accuracy_nb_train)\n",
    "\n",
    "# Print classification report for the test set\n",
    "classification_report_nb = classification_report(y_test_nb, y_pred_nb_test, target_names = [\"Sadness\", \"Joy\", \"Love\", \"Anger\", \"Fear\"])\n",
    "print(\"Classification Report on Test Set:\")\n",
    "print(classification_report_nb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "<b> (b) Logistic Regression model <b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create copies of the train and test data for Log Regression model\n",
    "X_train_lr = X_train_bal.copy()\n",
    "y_train_lr = y_train_bal.copy()\n",
    "\n",
    "X_test_lr = X_test.copy()\n",
    "y_test_lr = y_test.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 48 candidates, totalling 240 fits\n",
      "Best Parameters for Logistic Regression:\n",
      "{'cvec__max_df': 0.4, 'cvec__max_features': 5000, 'cvec__min_df': 2, 'cvec__ngram_range': (1, 2), 'lr__C': 0.2}\n",
      "Grid Search took 1043.01 seconds\n",
      "Accuracy Score on Test Set (Logistic Regression): 0.9438039777582059\n",
      "Accuracy Score on Training Set (Logistic Regression): 0.9683765841882921\n",
      "Classification Report on Test Set (Logistic Regression):\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "     Sadness       0.97      0.94      0.96     23630\n",
      "         Joy       0.97      0.93      0.95     26942\n",
      "        Love       0.81      0.98      0.89      5882\n",
      "       Anger       0.93      0.95      0.94     10919\n",
      "        Fear       0.92      0.96      0.94      8700\n",
      "\n",
      "    accuracy                           0.94     76073\n",
      "   macro avg       0.92      0.95      0.93     76073\n",
      "weighted avg       0.95      0.94      0.94     76073\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Create a pipeline with CountVectorizer and LogisticRegression\n",
    "pipeline_lr = Pipeline([\n",
    "    ('cvec', CountVectorizer(lowercase=True, stop_words='english')),\n",
    "    ('lr', LogisticRegression(max_iter=15000, random_state=42))\n",
    "])\n",
    "\n",
    "# Define parameter grid for grid search\n",
    "param_grid_lr = {\n",
    "    'cvec__max_features': [4000, 5000],\n",
    "    'cvec__min_df': [2, 3, 4],\n",
    "    'cvec__max_df': [0.40, 0.60],\n",
    "    'cvec__ngram_range': [(1, 2), (1, 3)],\n",
    "    'lr__C': [0.02, 0.2],\n",
    "}\n",
    "\n",
    "# Perform grid search with cross-validation\n",
    "grid_search_lr = GridSearchCV(pipeline_lr, param_grid_lr, cv=5, scoring='accuracy', verbose=1)\n",
    "start_time_lr = time.time()\n",
    "grid_search_lr.fit(X_train_lr, y_train_lr)\n",
    "end_time_lr = time.time()\n",
    "\n",
    "# Print best parameters\n",
    "best_params_lr = grid_search_lr.best_params_\n",
    "print(\"Best Parameters for Logistic Regression:\")\n",
    "print(best_params_lr)\n",
    "\n",
    "# Print computational time\n",
    "print(f\"Grid Search took {end_time_lr - start_time_lr:.2f} seconds\")\n",
    "\n",
    "# Print accuracy score for the test set\n",
    "y_pred_lr_test = grid_search_lr.best_estimator_.predict(X_test_lr)\n",
    "accuracy_lr_test = accuracy_score(y_test_lr, y_pred_lr_test)\n",
    "print(\"Accuracy Score on Test Set (Logistic Regression):\", accuracy_lr_test)\n",
    "\n",
    "# Print accuracy score for the training set\n",
    "y_pred_lr_train = grid_search_lr.best_estimator_.predict(X_train_lr)\n",
    "accuracy_lr_train = accuracy_score(y_train_lr, y_pred_lr_train)\n",
    "print(\"Accuracy Score on Training Set (Logistic Regression):\", accuracy_lr_train)\n",
    "\n",
    "# Print classification report for the test set\n",
    "classification_report_lr = classification_report(y_test_lr, y_pred_lr_test, target_names = [\"Sadness\", \"Joy\", \"Love\", \"Anger\", \"Fear\"])\n",
    "print(\"Classification Report on Test Set (Logistic Regression):\")\n",
    "print(classification_report_lr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "<b> (c) XGBoost Model </b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create copies of the train and test data for XG Boost model\n",
    "X_train_xg = X_train_bal.copy()\n",
    "y_train_xg = y_train_bal.copy()\n",
    "\n",
    "X_test_xg = X_test.copy()\n",
    "y_test_xg = y_test.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 108 candidates, totalling 540 fits\n",
      "Best Parameters:\n",
      "{'cvec__max_df': 0.8, 'cvec__max_features': 5000, 'cvec__min_df': 2, 'cvec__ngram_range': (1, 2), 'xgb__learning_rate': 0.2}\n",
      "Grid Search took 2010.38 seconds\n",
      "Accuracy Score on Test Set: 0.9266888383526349\n",
      "Accuracy Score on Training Set: 0.9475299594792654\n",
      "Classification Report on Test Set:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "     Sadness       0.95      0.91      0.93     23630\n",
      "         Joy       0.96      0.92      0.94     26942\n",
      "        Love       0.78      0.99      0.87      5882\n",
      "       Anger       0.93      0.92      0.92     10919\n",
      "        Fear       0.89      0.96      0.92      8700\n",
      "\n",
      "    accuracy                           0.93     76073\n",
      "   macro avg       0.90      0.94      0.92     76073\n",
      "weighted avg       0.93      0.93      0.93     76073\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Create a pipeline with CountVectorizer and XGBClassifier\n",
    "pipeline_xgb = Pipeline([\n",
    "    ('cvec', CountVectorizer(lowercase=True, stop_words= 'english')),\n",
    "    ('xgb', XGBClassifier(objective='multi:softmax', num_class=5, random_state=42))\n",
    "])\n",
    "\n",
    "# Define parameter grid for grid search\n",
    "param_grid_xgb = {\n",
    "    'cvec__max_features': [4000, 5000],\n",
    "    'cvec__min_df': [2, 3, 4],\n",
    "    'cvec__max_df': [0.4, 0.6, 0.8],\n",
    "    'cvec__ngram_range': [(1, 1), (1, 2), (1, 3)],\n",
    "    'xgb__learning_rate': [0.01, 0.2],  # you can add more hyperparameters for XGBoost here\n",
    "}\n",
    "\n",
    "# Perform grid search with cross-validation\n",
    "grid_search_xgb = GridSearchCV(pipeline_xgb, param_grid_xgb, cv=5, scoring='accuracy', verbose=1)\n",
    "start_time = time.time()\n",
    "grid_search_xgb.fit(X_train_xg, y_train_xg)  # assuming your training data is in X_train and y_train\n",
    "end_time = time.time()\n",
    "\n",
    "# Print best parameters\n",
    "best_params_xgb = grid_search_xgb.best_params_\n",
    "print(\"Best Parameters:\")\n",
    "print(best_params_xgb)\n",
    "\n",
    "# Print computational time\n",
    "print(f\"Grid Search took {end_time - start_time:.2f} seconds\")\n",
    "\n",
    "# Print accuracy score for the test set\n",
    "y_pred_xgb_test = grid_search_xgb.best_estimator_.predict(X_test_xg)  # assuming your test data is in X_test\n",
    "accuracy_xgb_test = accuracy_score(y_test_xg, y_pred_xgb_test)  # assuming your test labels are in y_test\n",
    "print(\"Accuracy Score on Test Set:\", accuracy_xgb_test)\n",
    "\n",
    "# Print accuracy score for the training set\n",
    "y_pred_xgb_train = grid_search_xgb.best_estimator_.predict(X_train_xg)\n",
    "accuracy_xgb_train = accuracy_score(y_train_xg, y_pred_xgb_train)\n",
    "print(\"Accuracy Score on Training Set:\", accuracy_xgb_train)\n",
    "\n",
    "# Print classification report for the test set\n",
    "classification_report_xgb = classification_report(y_test_xg, y_pred_xgb_test, target_names = [\"Sadness\", \"Joy\", \"Love\", \"Anger\", \"Fear\"])\n",
    "print(\"Classification Report on Test Set:\")\n",
    "print(classification_report_xgb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "<b> 4. Evaluate Classification Models </b>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "(a) Naive Bayes\n",
    "\n",
    "|Multinomial|Train Accuracy|Test Precision|Test Recall|Test F1-Score|Test Accuracy|Support|Time seconds|\n",
    "|---|---|---|---|---|---|---|---|\n",
    "|Class 0: Sadness|---|0.97|0.93|0.95|---|23630|---|\n",
    "|Class 1: Joy|---|0.97|0.90|0.94|---|26942|---|\n",
    "|Class 2: Love|---|0.74|0.96|0.84|---|5882|---|\n",
    "|Class 3: Anger|---|0.91|0.95|0.93|---|10919|---|\n",
    "|Class 4: Fear|---|0.88|0.96|0.92|---|8700|---|\n",
    "|Overall|0.95|---|---|---|0.93|76073|1497|\n",
    "\n",
    "<br>\n",
    "\n",
    "<br>\n",
    "\n",
    "(b) Logistic Regression\n",
    "\n",
    "|Multinomial|Train Accuracy|Test Precision|Test Recall|Test F1-Score|Test Accuracy|Support|Time seconds|\n",
    "|---|---|---|---|---|---|---|---|\n",
    "|Class 0: Sadness|---|0.97|0.94|0.96|---|23630|---|\n",
    "|Class 1: Joy|---|0.97|0.93|0.95|---|26942|---|\n",
    "|Class 2: Love|---|0.81|0.98|0.89|---|5882|---|\n",
    "|Class 3: Anger|---|0.93|0.95|0.94|---|10919|---|\n",
    "|Class 4: Fear|---|0.92|0.96|0.94|---|8700|---|\n",
    "|Overall|0.97|---|---|---|0.94|76073|1043|\n",
    "\n",
    "<br>\n",
    "\n",
    "(c) XG Boost\n",
    "\n",
    "|Multinomial|Train Accuracy|Test Precision|Test Recall|Test F1-Score|Test Accuracy|Support|Time seconds|\n",
    "|---|---|---|---|---|---|---|---|\n",
    "|Class 0: Sadness|---|0.95|0.91|0.93|---|23630|---|\n",
    "|Class 1: Joy|---|0.96|0.92|0.94|---|26942|---|\n",
    "|Class 2: Love|---|0.78|0.99|0.87|---|5882|---|\n",
    "|Class 3: Anger|---|0.93|0.92|0.92|---|10919|---|\n",
    "|Class 4: Fear|---|0.89|0.96|0.92|---|8700|---|\n",
    "|Overall|0.95|---|---|---|0.93|76073|2010|\n",
    "\n",
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Emotion Classification Models Evaluation\n",
    "\n",
    "We have built three models here to classify text into five different emotion classes: Sadness, Joy, Love, Anger, and Fear. The models are evaluated based on their F1-score, accuracy, and efficiency (time to run each model).\n",
    "\n",
    "## Evaluation Metrics\n",
    "\n",
    "1. **Accuracy**: This metric is particularly important in this case because we have a multi-class problem. A high accuracy means that the model is good at predicting the correct emotion class out of the five possible classes. \n",
    "\n",
    "2. **F1-Score**: This metric is crucial for this task because it balances precision and recall. In the context of emotion classification, precision means the percentage of correct predictions for a particular emotion out of all predictions for that emotion, while recall is the percentage of correct predictions for a particular emotion out of all actual instances of that emotion. The F1-score is particularly useful if we want to have a balance between identifying as many instances of each emotion as possible (high recall) and keeping the number of incorrect predictions low (high precision).\n",
    "\n",
    "3. **Efficiency**: This is important because we want the model to be able to process text quickly, especially if we’re dealing with large amounts of data. A model that takes too long to make predictions might not be practical for this use case.\n",
    "\n",
    "## Models Evaluation\n",
    "\n",
    "### Naive Bayes\n",
    "The model has a high overall accuracy and F1-score, which suggests that it’s good at identifying the correct emotion class. However, the F1-score for the ‘Love’ class is relatively low, which means the model might struggle to correctly identify this emotion. The model is also relatively efficient, with a runtime of 1497 seconds.\n",
    "\n",
    "### Logistic Regression\n",
    "This model has the highest overall accuracy and F1-score, suggesting that it’s the best at identifying the correct emotion class. It also has a high F1-score for all individual classes, including ‘Love’. Moreover, it’s more efficient than the Naive Bayes model, with a runtime of 1043 seconds.\n",
    "\n",
    "### XG Boost\n",
    "This model has a similar overall accuracy and F1-score to the Naive Bayes model. However, it’s less efficient, with a runtime of 2010 seconds. The F1-score for the ‘Love’ class is higher than for the Naive Bayes model, but lower than for the Logistic Regression model.\n",
    "\n",
    "### Based on these evaluations, the Logistic Regression model seems to be the best choice for this task. It has the highest accuracy and F1-scores, and it’s also relatively efficient. We will move on next to try a Multinomial Classifier RNN with Glove word embeddings Model as semantic understanding can be crucial for emotion classification tasks"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
